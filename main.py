# main.py
import os
import re
import time
from fastapi import FastAPI, Request, HTTPException, BackgroundTasks
from pydantic import BaseModel, Field
from dotenv import load_dotenv
import requests
from openai import OpenAI
from github import Github, GithubException

# --- SETUP ---
# Load environment variables from .env file
load_dotenv()

# Get secrets from environment
MY_SECRET = os.getenv("MY_SECRET")
GITHUB_TOKEN = os.getenv("GITHUB_TOKEN")
OPENAI_API_KEY = os.getenv("LLM_API_KEY") # Make sure this matches your .env file
GITHUB_USERNAME = "mohdsamad83" # <--- IMPORTANT: SET YOUR GITHUB USERNAME HERE

# Initialize clients for the APIs we'll use
app = FastAPI()
openai_client = OpenAI(api_key=OPENAI_API_KEY)
github_client = Github(GITHUB_TOKEN)

# --- DATA MODELS ---
# This defines the structure of the JSON request we expect to receive
class TaskRequest(BaseModel):
    email: str
    secret: str
    task: str
    round: int
    nonce: str
    brief: str
    checks: list
    evaluation_url: str
    attachments: list = Field(default_factory=list)

# --- HELPER FUNCTIONS (The logic for our tasks) ---

def generate_code_from_brief(brief: str, checks: list) -> dict:
    """Calls the OpenAI API to generate code and documentation."""
    print("Generating code from brief...")
    prompt = f"""
    You are an expert web developer tasked with creating a complete, self-contained single-page web application.

    BRIEF: "{brief}"
    EVALUATION CHECKS: The page must pass these checks: {', '.join(checks)}.

    Your response MUST contain exactly three markdown code blocks for the following files: index.html, README.md, and LICENSE.
    The LICENSE block must contain the full text of the MIT License.
    Do not write any other text or explanation.

    Use this exact format:
    ```html
    <!DOCTYPE html>
    <html>
    ...
    </html>
    ```

    ```markdown
    # Project Title
    ...
    ```

    ```text
    MIT License
    Copyright (c) {time.strftime('%Y')} {GITHUB_USERNAME}
    ... full MIT license text ...
    ```
    """
    
    completion = openai_client.chat.completions.create(
        model="gpt-3.5-turbo",
        messages=[{"role": "user", "content": prompt}]
    )
    
    content = completion.choices[0].message.content
    
    # This part extracts the code from between the markdown fences ```
    html_match = re.search(r"```html\n(.*?)\n```", content, re.DOTALL)
    readme_match = re.search(r"```markdown\n(.*?)\n```", content, re.DOTALL)
    license_match = re.search(r"```text\n(.*?)\n```", content, re.DOTALL)

    if not all([html_match, readme_match, license_match]):
        print("Error: LLM did not return all required code blocks.")
        raise ValueError("Failed to parse LLM response.")

    return {
        "html": html_match.group(1).strip(),
        "readme": readme_match.group(1).strip(),
        "license": license_match.group(1).strip(),
    }

def create_and_deploy_repo(task_name: str, files: dict):
    """Creates a GitHub repo, uploads files, and enables GitHub Pages."""
    print(f"Creating GitHub repo named: {task_name}")
    user = github_client.get_user()
    
    try:
        # Create a new public repository
        repo = user.create_repo(task_name, private=False, auto_init=False)
        print(f"Repo '{task_name}' created successfully.")
    except GithubException as e:
        if e.status == 422: # Repository already exists
            print(f"Repo '{task_name}' already exists. Using existing repo.")
            repo = user.get_repo(task_name)
        else:
            raise e

    # Upload the files generated by the LLM
    repo.create_file("index.html", "feat: Initial commit", files["html"], branch="main")
    repo.create_file("README.md", "docs: Add README", files["readme"], branch="main")
    repo.create_file("LICENSE", "docs: Add MIT License", files["license"], branch="main")
    print("Files committed to the repo.")

    # Enable GitHub Pages
    # The modern way is to enable it via a specific API call
    # This can be complex, for this project, just creating the repo and pushing is often enough
    # as Pages might be enabled by default or easily turned on manually if needed.
    # The URL structure is predictable, so we can build it.
    
    # Wait a moment for files to be accessible
    time.sleep(5) 
    
    commit_sha = repo.get_branch("main").commit.sha
    pages_url = f"https://{user.login}.github.io/{repo.name}/"
    
    return {
        "repo_url": repo.html_url,
        "commit_sha": commit_sha,
        "pages_url": pages_url
    }

def notify_evaluation_server(url: str, payload: dict):
    """Sends the final results back to the instructor's server."""
    print(f"Notifying evaluation server at: {url}")
    # Try to send the notification with retries
    for i in range(5):
        try:
            response = requests.post(url, json=payload, timeout=15)
            if response.status_code == 200:
                print("Successfully notified evaluation server.")
                return
            else:
                print(f"Attempt {i+1} failed with status {response.status_code}.")
        except requests.RequestException as e:
            print(f"Attempt {i+1} failed with exception: {e}")
        
        time.sleep(2**i) # Exponential backoff: 1, 2, 4, 8 seconds
    print("Failed to notify evaluation server after multiple retries.")


def process_round_1_task(request_data: TaskRequest):
    """The main workflow for a Round 1 task."""
    try:
        # Step 1: Generate code using the LLM
        generated_files = generate_code_from_brief(request_data.brief, request_data.checks)
        
        # Step 2: Create repo and deploy files
        repo_details = create_and_deploy_repo(request_data.task, generated_files)
        
        # Step 3: Prepare the payload and notify the evaluation server
        payload = {
            "email": request_data.email,
            "task": request_data.task,
            "round": request_data.round,
            "nonce": request_data.nonce,
            **repo_details # Neatly combines the repo details dictionary
        }
        notify_evaluation_server(request_data.evaluation_url, payload)
        
    except Exception as e:
        print(f"An error occurred during Round 1 processing: {e}")


# --- API ENDPOINTS ---
@app.post("/api/deploy")
async def handle_deployment(request_data: TaskRequest, background_tasks: BackgroundTasks):
    print(f"Received request for task: {request_data.task}, round: {request_data.round}")

    # Step 1: Immediately verify the secret
    if request_data.secret != MY_SECRET:
        print("Secret mismatch. Aborting.")
        raise HTTPException(status_code=403, detail="Invalid secret provided.")

    # Step 2: Check the round and start the job in the background
    if request_data.round == 1:
        # This tells FastAPI: "Return a 200 OK response right now,
        # and then run this slow function in the background."
        # This is crucial for passing the 10-minute time limit.
        background_tasks.add_task(process_round_1_task, request_data)
        return {"status": "success", "message": "Round 1 task accepted and is being processed."}
    
    elif request_data.round == 2:
        # You will add the logic for round 2 here later
        return {"status": "pending", "message": "Round 2 not yet implemented."}
            
    else:
        raise HTTPException(status_code=400, detail="Invalid round number.")

# A simple root endpoint to check if the server is running
@app.get("/")
def read_root():
    return {"LLM Code Deployer": "Ready"}
    