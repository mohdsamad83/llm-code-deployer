# main.py
import os
import re
import time
from fastapi import FastAPI, HTTPException, BackgroundTasks
from pydantic import BaseModel, Field
from dotenv import load_dotenv
import requests
from openai import OpenAI
from github import Github, GithubException

## --- 1. SETUP AND CONFIGURATION ---

# Load environment variables from your .env file
load_dotenv()

# Fetch your secret keys and username from the environment
MY_SECRET = os.getenv("MY_SECRET")
GITHUB_TOKEN = os.getenv("GITHUB_TOKEN")
OPENAI_API_KEY = os.getenv("LLM_API_KEY") # Ensure this name matches your .env file
# ‚ùó IMPORTANT: Replace this with your actual GitHub username in quotes
GITHUB_USERNAME = "mohdsamad83"  # e.g., "your-github-username"

# Check if all required secrets are set
if not all([MY_SECRET, GITHUB_TOKEN, OPENAI_API_KEY, GITHUB_USERNAME != "YOUR_GITHUB_USERNAME"]):
    raise ValueError("One or more required environment variables or GITHUB_USERNAME are not set.")

# Initialize clients for the APIs we'll use
app = FastAPI()
openai_client = OpenAI(api_key=OPENAI_API_KEY)
github_client = Github(GITHUB_TOKEN)


## --- 2. DATA MODELS ---

# Defines the structure of the JSON request we expect to receive using Pydantic.
# This ensures the incoming data is valid.
class TaskRequest(BaseModel):
    email: str
    secret: str
    task: str
    round: int
    nonce: str
    brief: str
    checks: list
    evaluation_url: str
    attachments: list = Field(default_factory=list)


## --- 3. HELPER FUNCTIONS (The Core Logic) ---

def generate_code_from_brief(brief: str, checks: list) -> dict:
    """Calls the OpenAI API to generate code and documentation based on a prompt."""
    print("ü§ñ Calling OpenAI to generate code...")
    # This detailed prompt guides the LLM to produce the exact output format we need.
    prompt = f"""
    You are an expert web developer creating a complete, self-contained single-page web application.

    BRIEF: "{brief}"
    EVALUATION CHECKS: The final page must satisfy these requirements: {', '.join(checks)}.

    Your response MUST contain exactly three markdown code blocks for the following files: index.html, README.md, and LICENSE.
    The LICENSE block must contain the full text of the MIT License.
    Do not write any other text, explanation, or introductions.

    Use this exact format:
    ```html
    <!DOCTYPE html>
    <html lang="en">
    ...
    </html>
    ```

    ```markdown
    # Project Title
    A brief summary of the project.
    ```

    ```text
    MIT License
    Copyright (c) {time.strftime('%Y')} {GITHUB_USERNAME}
    Permission is hereby granted, free of charge, to any person obtaining a copy...
    ... (the rest of the full MIT license text) ...
    ```
    """
    
    try:
        completion = openai_client.chat.completions.create(
            model="gpt-4o-mini", # Using a cost-effective and capable model
            messages=[{"role": "user", "content": prompt}]
        )
        content = completion.choices[0].message.content
    except Exception as e:
        print(f"‚ùå OpenAI API call failed: {e}")
        raise

    # Use regular expressions to safely extract content from each code block
    html_match = re.search(r"```html\n(.*?)\n```", content, re.DOTALL)
    readme_match = re.search(r"```markdown\n(.*?)\n```", content, re.DOTALL)
    license_match = re.search(r"```text\n(.*?)\n```", content, re.DOTALL)

    if not all([html_match, readme_match, license_match]):
        print("‚ùå Error: LLM response did not contain all three required code blocks.")
        raise ValueError("Failed to parse LLM response. The output format was incorrect.")

    print("‚úÖ Code generated successfully.")
    return {
        "html": html_match.group(1).strip(),
        "readme": readme_match.group(1).strip(),
        "license": license_match.group(1).strip(),
    }

def create_and_deploy_repo(task_name: str, files: dict) -> dict:
    """Creates a GitHub repo, uploads files, and constructs the Pages URL."""
    print(f"üêô Accessing GitHub to create repo: {task_name}")
    user = github_client.get_user()
    
    try:
        # Create a new public repository.
        repo = user.create_repo(task_name, private=False, auto_init=False)
        print(f"‚úÖ Repo '{task_name}' created.")
    except GithubException as e:
        # If the repo already exists (e.g., from a failed previous run), use it.
        if e.status == 422:
            print(f"‚ö†Ô∏è Repo '{task_name}' already exists. Proceeding with existing repo.")
            repo = user.get_repo(task_name)
        else:
            print(f"‚ùå GitHub API error: {e}")
            raise

    # Upload the files generated by the LLM to the main branch
    repo.create_file("index.html", "feat: Initial application structure", files["html"], branch="main")
    repo.create_file("README.md", "docs: Add project README", files["readme"], branch="main")
    repo.create_file("LICENSE", "docs: Add MIT License", files["license"], branch="main")
    print("‚úÖ Files committed to the repo.")

    # We need to wait a few seconds for GitHub to process the commit
    time.sleep(5) 
    
    commit_sha = repo.get_branch("main").commit.sha
    pages_url = f"https://{user.login}.github.io/{repo.name}/"
    
    return {
        "repo_url": repo.html_url,
        "commit_sha": commit_sha,
        "pages_url": pages_url
    }

def notify_evaluation_server(url: str, payload: dict):
    """Sends the final results back to the instructor's server with retries."""
    print(f"üì® Notifying evaluation server at: {url}")
    
    # Retry with exponential backoff (1, 2, 4, 8 seconds) if the server is busy
    for i in range(4):
        try:
            response = requests.post(url, json=payload, timeout=20)
            if response.status_code == 200:
                print("‚úÖ Successfully notified evaluation server.")
                return
            else:
                print(f"‚ö†Ô∏è Attempt {i+1} failed with status {response.status_code}. Retrying...")
        except requests.RequestException as e:
            print(f"‚ö†Ô∏è Attempt {i+1} failed with network error: {e}. Retrying...")
        
        time.sleep(2**i)
    
    print("‚ùå Failed to notify evaluation server after multiple retries.")

def process_round_1_task(request_data: TaskRequest):
    """The main workflow that runs in the background for a Round 1 task."""
    print(f"üöÄ Starting Round 1 processing for task: {request_data.task}")
    try:
        # Step 1: Generate code using the LLM
        generated_files = generate_code_from_brief(request_data.brief, request_data.checks)
        
        # Step 2: Create repo and deploy files to GitHub
        repo_details = create_and_deploy_repo(request_data.task, generated_files)
        
        # Step 3: Prepare the final JSON payload
        payload = {
            "email": request_data.email,
            "task": request_data.task,
            "round": request_data.round,
            "nonce": request_data.nonce,
            "repo_url": repo_details["repo_url"],
            "commit_sha": repo_details["commit_sha"],
            "pages_url": repo_details["pages_url"],
        }
        
        # Step 4: Send payload to the evaluation server
        notify_evaluation_server(request_data.evaluation_url, payload)
        
    except Exception as e:
        print(f"‚ùå An unrecoverable error occurred during Round 1 processing: {e}")
    print(f"üèÅ Finished processing task: {request_data.task}")


## --- 4. API ENDPOINTS (The Server's "Doors") ---

@app.post("/api/deploy")
async def handle_deployment(request_data: TaskRequest, background_tasks: BackgroundTasks):
    """This is the main endpoint that receives requests from the instructor."""
    print(f"Received request for task: {request_data.task}, round: {request_data.round}")

    # Immediately verify the secret. If it's wrong, reject the request.
    if request_data.secret != MY_SECRET:
        print("‚ùå Secret mismatch. Aborting.")
        raise HTTPException(status_code=403, detail="Invalid secret provided.")

    # Handle the task based on the round number
    if request_data.round == 1:
        # Crucially, we add the slow work as a background task.
        # This allows us to return a 200 OK response immediately.
        background_tasks.add_task(process_round_1_task, request_data)
        return {"status": "success", "message": "Round 1 task accepted and is being processed in the background."}
    
    elif request_data.round == 2:
        # Placeholder for future logic
        return {"status": "pending", "message": "Round 2 is not yet implemented."}
            
    else:
        raise HTTPException(status_code=400, detail="Invalid round number specified.")

@app.get("/")
def read_root():
    """A simple endpoint to confirm that the server is running."""
    return {"LLM Code Deployer API": "Online and ready!"}